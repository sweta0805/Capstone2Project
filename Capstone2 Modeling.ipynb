{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_data/datascientist_data_step4_features.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Test Datasets\n",
    "When fitting models, we would like to ensure two things:\n",
    "\n",
    "We have found the best model (in terms of model parameters).\n",
    "The model is highly likely to generalize i.e. perform well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of splitting data into Training/testing sets\n",
    "We built our model with the requirement that the model fit the data well.\n",
    "As a side-effect, the model will fit THIS dataset well. What about new data?\n",
    "We wanted the model for predictions, right?\n",
    "One simple solution, leave out some data (for testing) and train the model on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Est_Salary']\n",
    "X = df.drop(['Est_Salary'],axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=47)    \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use K Fold cross validation to measure accuracy of our Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Model\n",
    "The model has some hyperparameters we can tune for hopefully better performance. For tuning the parameters of model, i m using mix of cross-validation and grid search. In Logistic Regression, the most important parameter to tune is the regularization parameter C. Note that the regularization parameter is not always part of the logistic regression model.\n",
    "\n",
    "The regularization parameter is used to control for unlikely high regression coefficients, and in other cases can be used when data is sparse, as a method of feature selection.\n",
    "\n",
    "Now implement some code to perform model tuning and selecting the regularization parameter $C$.\n",
    "\n",
    "We use the following cv_score function to perform K-fold cross-validation and apply a scoring function to each test fold. In this incarnation we use accuracy score as the default scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "cross_val_score(LinearRegression(), X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "ridge = Ridge(alpha=1.0, normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "#ridge.score(X_test, y_test)\n",
    "#accuracy\n",
    "accuracy = ridge.score(X_train,y_train)\n",
    "print('\\nModel Accuracy:- ', accuracy*100,'%')\n",
    "\n",
    "accuracy = ridge.score(X_test,y_test)\n",
    "print('\\nTest Data Accuracy:- ', accuracy*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression gives us accuracy of 24% on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, ypred):\n",
    "    \"\"\"Mean absolute error.\n",
    "    \n",
    "    Calculate the mean absolute error of the arguments\n",
    "\n",
    "    Arguments:\n",
    "    y -- the observed values\n",
    "    predction -- the predicted values\n",
    "    What exactly does ‘ERROR’ in this metric mean ?\n",
    "    Prediction Error => Actual Value - Predicted Value\n",
    "    \"\"\"\n",
    "    abs_error = np.abs(y - ypred)\n",
    "    mae = np.mean(abs_error)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(y_test,ridge_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean absolute percentage Accuracy (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(ridge_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Mean absolute percentage Accuracy (MAPE):', round(accuracy, 2), '%.')\n",
    "print('Mean absolute percentage error (MAPE):', round(np.mean(mape), 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "MSE = mean_squared_error(y_test,ridge_pred)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this back to our measurement space, we often take the square root, to form the root mean square error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Squared Error',np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y_test, ridge_pred, c='green')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "#plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test-ridge_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(residuals, bins = 10) #histplot\n",
    "plt.title('Error Terms', fontsize=20)           \n",
    "plt.xlabel('Residuals', fontsize = 15)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "lasso = Lasso(alpha=0.1, normalize=True)\n",
    "#fit the model\n",
    "lasso.fit(X_train, y_train)\n",
    "#make a prediction\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "\n",
    "#check accuracy\n",
    "accuracy = lasso.score(X_train,y_train)\n",
    "print('\\nModel Accuracy:- ', accuracy*100,'%')\n",
    "\n",
    "accuracy = lasso.score(X_test,y_test)\n",
    "print('\\nTest Data Accuracy:- ', accuracy*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(y_test,lasso_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean absolute percentage Accuracy (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "errors = abs(lasso_pred - y_test)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Mean absolute percentage Accuracy (MAPE):', round(accuracy, 2), '%.')\n",
    "print('Mean absolute percentage error (MAPE):', round(np.mean(mape), 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "MSE = mean_squared_error(y_test,lasso_pred)\n",
    "\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Squared Error',np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test-lasso_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(residuals, bins = 10) #histplot\n",
    "plt.title('Error Terms', fontsize=20)           \n",
    "plt.xlabel('Residuals', fontsize = 15)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train = sm.add_constant(X_train) # required if constant expected\n",
    "est = sm.OLS(y_train,X_train).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_test = sm.add_constant(X_test) # required if constant expected\n",
    "est = sm.OLS(y_test,X_test).fit() # fit model\n",
    "predictions = est.predict() # get predicted values\n",
    "print(est.summary()) # prints full regression results\n",
    "print(\"\\nAverage error: {:.2f}.\".format(math.sqrt(est.mse_resid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training and test (hold-out) set\n",
    "Train on the training set, and test for accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using Linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#accuracy\n",
    "accuracy = model.score(X_train,y_train)\n",
    "print('\\nModel Accuracy:- ', accuracy*100,'%')\n",
    "\n",
    "accuracy = model.score(X_test,y_test)\n",
    "print('\\nTest Data Accuracy:- ', accuracy*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "linear_pred = model.predict(X_test)#y_pred\n",
    "# Calculate the absolute errors\n",
    "errors = abs(linear_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Mean absolute percentage Accuracy (MAPE):', round(accuracy, 2), '%.')\n",
    "print('Mean absolute percentage error (MAPE):', round(np.mean(mape), 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "MSE = mean_squared_error(y_test,linear_pred)\n",
    "\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Squared Error',np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test-linear_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(residuals, bins = 10) #histplot\n",
    "plt.title('Error Terms', fontsize=20)           \n",
    "plt.xlabel('Residuals', fontsize = 15)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 3 Random Samples for prediction; from unseen data\n",
    "Sample_X_test = X_test.sample(n=3, random_state=2)\n",
    "#prediction for 3 samples\n",
    "pred = model.predict(Sample_X_test)\n",
    "Sample_X_test\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Featured 3 sample data used for prediction\")\n",
    "pd.DataFrame({'First Sample' : df.iloc[1334],\n",
    "              'Second Sample' : df.iloc[3735],\n",
    "              'Third Sample' : df.iloc[116]}).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sample_Y_test = y_test.sample(n=3, random_state=2)\n",
    "Sample_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"\\n1. Safe Model Confusion Matrix for Training Data :- \\n\")\n",
    "pd.crosstab(Sample_Y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review the confusion matrix if avg salary is 111 but model predict it 96, if avg salary is 131 but model predict it 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert random sample to user readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 3 Random Samples for prediction; from unseen data\n",
    "pred1 = X_test.sample(n=3, random_state=2)\n",
    "print(\"Creating three random sample from dataset\")\n",
    "print(\"\\nBelow test data is unseen by Model during it's training\")\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
